{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': ['0203_food.txt-tfidf'],\n",
       " 'food': ['0201_food.txt-tfidf'],\n",
       " 'politics': ['0105_food.txt-tfidf',\n",
       "  '0101_sports.txt-tfidf',\n",
       "  '0117_sports.txt-tfidf',\n",
       "  '0306_science.txt-tfidf',\n",
       "  '0111_sports.txt-tfidf',\n",
       "  '0502_business.txt-tfidf',\n",
       "  '0202_food.txt-tfidf',\n",
       "  '0114_sports.txt-tfidf',\n",
       "  '0108_sports.txt-tfidf',\n",
       "  '0520_politics.txt-tfidf',\n",
       "  '0116_food.txt-tfidf',\n",
       "  '0302_science.txt-tfidf'],\n",
       " 'science': ['0118_sports.txt-tfidf',\n",
       "  '0518_business.txt-tfidf',\n",
       "  '0515_politics.txt-tfidf',\n",
       "  '0104_sports.txt-tfidf',\n",
       "  '0305_tech.txt-tfidf',\n",
       "  '0310_science.txt-tfidf',\n",
       "  '0109_sports.txt-tfidf',\n",
       "  '0409_science.txt-tfidf',\n",
       "  '0519_business.txt-tfidf',\n",
       "  '0309_science.txt-tfidf',\n",
       "  '0420_tech.txt-tfidf',\n",
       "  '0313_tech.txt-tfidf',\n",
       "  '0312_tech.txt-tfidf',\n",
       "  '0315_science.txt-tfidf',\n",
       "  '0419_science.txt-tfidf',\n",
       "  '0415_tech.txt-tfidf',\n",
       "  '0216_food.txt-tfidf',\n",
       "  '0217_food.txt-tfidf',\n",
       "  '0319_tech.txt-tfidf',\n",
       "  '0318_tech.txt-tfidf',\n",
       "  '0406_science.txt-tfidf',\n",
       "  '0106_sports.txt-tfidf',\n",
       "  '0314_tech.txt-tfidf',\n",
       "  '0303_science.txt-tfidf',\n",
       "  '0506_business.txt-tfidf',\n",
       "  '0501_business.txt-tfidf',\n",
       "  '0211_food.txt-tfidf',\n",
       "  '0514_business.txt-tfidf',\n",
       "  '0120_sports.txt-tfidf',\n",
       "  '0513_business.txt-tfidf',\n",
       "  '0413_science.txt-tfidf',\n",
       "  '0205_sports.txt-tfidf',\n",
       "  '0416_science.txt-tfidf',\n",
       "  '0115_food.txt-tfidf',\n",
       "  '0112_sports.txt-tfidf',\n",
       "  '0512_business.txt-tfidf',\n",
       "  '0405_tech.txt-tfidf',\n",
       "  '0103_sports.txt-tfidf',\n",
       "  '0206_food.txt-tfidf',\n",
       "  '0207_food.txt-tfidf',\n",
       "  '0403_science.txt-tfidf',\n",
       "  '0507_business.txt-tfidf',\n",
       "  '0404_science.txt-tfidf',\n",
       "  '0214_food.txt-tfidf',\n",
       "  '0418_science.txt-tfidf',\n",
       "  '0301_science.txt-tfidf',\n",
       "  '0517_business.txt-tfidf',\n",
       "  '0220_sports.txt-tfidf',\n",
       "  '0308_science.txt-tfidf',\n",
       "  '0411_science.txt-tfidf',\n",
       "  '0311_tech.txt-tfidf',\n",
       "  '0408_science.txt-tfidf',\n",
       "  '0119_sports.txt-tfidf',\n",
       "  '0414_science.txt-tfidf',\n",
       "  '0401_science.txt-tfidf',\n",
       "  '0503_business.txt-tfidf',\n",
       "  '0208_food.txt-tfidf',\n",
       "  '0209_food.txt-tfidf',\n",
       "  '0504_business.txt-tfidf',\n",
       "  '0110_food.txt-tfidf',\n",
       "  '0511_business.txt-tfidf',\n",
       "  '0304_science.txt-tfidf',\n",
       "  '0516_business.txt-tfidf',\n",
       "  '0307_science.txt-tfidf',\n",
       "  '0113_sports.txt-tfidf',\n",
       "  '0509_business.txt-tfidf',\n",
       "  '0402_science.txt-tfidf',\n",
       "  '0417_science.txt-tfidf',\n",
       "  '0102_sports.txt-tfidf',\n",
       "  '0320_science.txt-tfidf',\n",
       "  '0215_sports.txt-tfidf',\n",
       "  '0107_sports.txt-tfidf',\n",
       "  '0505_politics.txt-tfidf',\n",
       "  '0212_food.txt-tfidf',\n",
       "  '0213_food.txt-tfidf',\n",
       "  '0412_science.txt-tfidf',\n",
       "  '0508_business.txt-tfidf',\n",
       "  '0410_tech.txt-tfidf',\n",
       "  '0510_politics.txt-tfidf',\n",
       "  '0210_sports.txt-tfidf',\n",
       "  '0218_food.txt-tfidf',\n",
       "  '0219_food.txt-tfidf',\n",
       "  '0317_tech.txt-tfidf',\n",
       "  '0316_tech.txt-tfidf',\n",
       "  '0407_science.txt-tfidf'],\n",
       " 'sports': ['idf'],\n",
       " 'tech': ['0204_food.txt-tfidf']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "# Ignore FutureWarnings, there is a function call within the sklearn library that is using an outdated pandas method (I think)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Load the data from the Excel file\n",
    "data = pd.read_excel(\"tdf-vectors.xlsx\")\n",
    "\n",
    "# Transpose the data so that documents are rows and words are columns\n",
    "transposed_data = data.set_index(\"Unnamed: 0\").transpose()\n",
    "\n",
    "# Specify the centroid document names and extract their TF-IDF vectors\n",
    "centroid_names = [\n",
    "    \"9901_sports.txt-tfidf\", \n",
    "    \"9902_food.txt-tfidf\", \n",
    "    \"9903_tech.txt-tfidf\", \n",
    "    \"9904_science.txt-tfidf\", \n",
    "    \"9905_business.txt-tfidf\", \n",
    "    \"9906_politics.txt-tfidf\"\n",
    "]\n",
    "centroids = transposed_data.loc[centroid_names]\n",
    "\n",
    "# Prepare the data for clustering (excluding the centroids themselves)\n",
    "data_for_clustering = transposed_data.drop(centroid_names)\n",
    "\n",
    "# Initialize KMeans with the specified centroids and fit the model to the data\n",
    "kmeans = KMeans(n_clusters=6, init=centroids, n_init=1)\n",
    "labels = kmeans.fit_predict(data_for_clustering)\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "data_for_clustering['Cluster'] = labels\n",
    "\n",
    "# Create a mapping from cluster number to cluster name\n",
    "cluster_mapping = {\n",
    "    0: \"sports\",\n",
    "    1: \"food\",\n",
    "    2: \"tech\",\n",
    "    3: \"science\",\n",
    "    4: \"business\",\n",
    "    5: \"politics\"\n",
    "}\n",
    "\n",
    "#  Replace cluster numbers with names in the dataframe\n",
    "data_for_clustering['Cluster'] = data_for_clustering['Cluster'].map(cluster_mapping)\n",
    "\n",
    "# Create a simplified dataframe showing the document and its cluster\n",
    "document_clusters = data_for_clustering[['Cluster']].reset_index()\n",
    "document_clusters.columns = ['Document', 'Cluster']\n",
    "\n",
    "# At this point, 'document_clusters' contains the documents with their respective cluster names.\n",
    "\n",
    "# Group by 'Cluster' and create a dictionary where keys are cluster names and values are lists of documents\n",
    "clustered_documents = {}\n",
    "for cluster, group in document_clusters.groupby('Cluster'):\n",
    "    clustered_documents[cluster] = list(group['Document'])\n",
    "\n",
    "# Display documents for each cluster \n",
    "clustered_document_samples = {cluster: docs for cluster, docs in clustered_documents.items()}\n",
    "clustered_document_samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
